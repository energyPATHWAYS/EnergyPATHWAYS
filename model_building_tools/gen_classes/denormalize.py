#!/usr/bin/env python

from __future__ import print_function
import click
from glob import glob
import os
import pandas as pd
import shutil
import pdb

from energyPATHWAYS.generated.text_mappings import MappedCols

from postgres import (find_key_col, find_parent_col, mkdirs, Simple_mapping_tables,
                       Tables_without_classes, Tables_to_ignore)

def _read_csv(pathname, tbl_name):
    # Avoid reading empty strings as nan
    str_cols = MappedCols.get(tbl_name, [])
    converters = {col: str for col in str_cols}
    converters['sensitivity'] = str

    df = pd.read_csv(pathname, index_col=None, converters=converters)
    return df

def get_key_col(dbdir, table_name):
    path = os.path.join(dbdir,  table_name + '.csv')
    df = _read_csv(path, table_name)
    key_col = find_key_col(table_name, df.columns)
    return key_col

def denormalize(dbdir, outdir, table_name, child_name):
    parentPath = os.path.join(dbdir,  table_name + '.csv')
    childPath  = os.path.join(dbdir,  child_name + '.csv')
    mergedPath = os.path.join(outdir, table_name + '.csv')

    # if not force and os.path.exists(mergedPath):
    #     mtime = os.path.getmtime(mergedPath)
    #     if os.path.getmtime(parentPath) < mtime and os.path.getmtime(parentPath) < mtime:
    #         print("Already merged table", table_name)
    #         return

    print("Merging", table_name)

    parentDF = _read_csv(parentPath, table_name)
    childDF  = _read_csv(childPath,  child_name)

    # In these cases, the child table already holds the desired result
    if table_name in ['Geographies', 'OtherIndexes', 'GeographiesSpatialJoin']:
        childDF.to_csv(mergedPath, index=None)
        return {'data_table': True}

    if 'source' not in parentDF.columns:
        parentDF['source'] = None
    if 'notes' not in parentDF.columns:
        parentDF['notes'] = None

    key_col = find_key_col(table_name, parentDF.columns)
    par_col = find_parent_col(child_name, childDF.columns)

    if not (key_col and par_col):
        if not key_col:
            print("*** Table {}: key_col not found".format(table_name))

        if not par_col:
            print("*** Table {}: par_col not found".format(table_name))

        return None

    if key_col == par_col:
        # if the parent and child match on a column with the same name, we
        # rename the child column before merging so "merge" doesn't rename them.
        new_col = 'CHILD_' + par_col
        childDF.rename(index=str, columns={par_col: new_col}, inplace=True)
        par_col = new_col

    if table_name in ('DemandTechsCapitalCost', 'DemandTechsInstallationCost'):
        parentDF['new_or_replacement'] = 'new'

    merged = pd.merge(parentDF, childDF, left_on=key_col, right_on=par_col, how='left')

    # Drop rows for totals whose values are zeros; these are regenerated by the model.
    # We do this first since it eliminates many rows in these two large tables.
    if table_name in ('DemandEnergyDemands', 'DemandServiceDemands'):
        zero_totals = merged.query('input_type == "total" and value == 0')
        merged.drop(zero_totals.index, axis=0, inplace=True)

    # Missing child data records show up in the merged DF with the parent column
    # as NaN. We set the rest of the columns from the child record to '_missing'
    child_cols = list(set(childDF.columns) - set([par_col]))

    # dropped this for now
    # merged.loc[merged[par_col].isnull(), child_cols] = '_missing'

    # drop the redundant parent column and child id column, if present
    to_drop = [par_col]
    if 'id' in child_cols:
        to_drop.append('id')
        child_cols.remove('id')

    merged.drop(to_drop, axis=1, inplace=True)

    # remove any duplicate rows. This is cleaning up errors that existed in the database
    merged = merged.drop_duplicates()

    # try to sort things as best we can
    sort_order = ['name', 'sector', 'subsector', 'node', 'sensitivity', 'gau', 'oth_1', 'oth_2', 'final_energy', 'demand_technology', 'year', 'vintage', 'day_type', 'month', 'hour', 'weather_datetime']
    if key_col not in sort_order:
        sort_order = [key_col] + sort_order
    all_nulls = merged.isnull().all()
    columns_that_exist = [col for col in sort_order if col in merged.columns and not all_nulls[col]]
    new_col_order = [key_col] + [col for col in merged.columns if col != key_col]  # put the key column first
    if columns_that_exist:
        merged = merged.set_index(columns_that_exist).sort_index().reset_index()[new_col_order]

    merged.to_csv(mergedPath, index=None)

    md = gen_metadata(key_col, child_cols)
    return md

def gen_metadata(key_col, cols):
    """
    Create the metadata for the merged table
    """
    md = {'key_col' : key_col,
          'df_cols' : cols}

    if 'sensitivity' in cols:
        md['lowcase_cols'] = ['sensitivity']

    drop_cols = []
    for col in ('source', 'notes'):
        if col in cols:
            drop_cols.append(col)

    if drop_cols:
        md['drop_cols'] = drop_cols

    return md

def gen_database_file(metadata_file, metadata, classname):
    classname = classname or os.path.basename(metadata_file).split('.')[0]
    with open(metadata_file, 'w') as f:
        f.write('''from csvdb import CsvMetadata, CsvDatabase
from .text_mappings import MappedCols

_Metadata = [
''')
        for tbl_name in sorted(metadata.keys()):
            md = metadata[tbl_name]
            attrs = ['{!r}'.format(tbl_name)] + (['{}={!r}'.format(key, value) for key, value in md.items()] if md else [])
            spacing = ',\n                '
            f.write('    CsvMetadata({}),\n'.format(spacing.join(attrs)))

        f.write(''']

class {classname}(CsvDatabase):
    def __init__(self, pathname=None, load=True, output_tables=False, compile_sensitivities=False, tables_to_not_load=None):
        super({classname}, self).__init__(
            metadata=_Metadata,  
            pathname=pathname, 
            load=load, 
            mapped_cols=MappedCols,
            output_tables=output_tables, 
            compile_sensitivities=compile_sensitivities, 
            tables_to_not_load=tables_to_not_load, 
            tables_without_classes={without_classes}, 
            tables_to_ignore={ignore})
'''.format(classname=classname, without_classes=Tables_without_classes, ignore=Tables_to_ignore))

def combine_techs(outdir):
    # Combine the files StorageTechsCapacityCost, StorageTechsEnergyCapitalCost, and
    # SupplyTechsCapitalCost. The resulting file will just be SupplyTechsCapitalCost.

    SupplyTechsCapitalCost = _read_csv(os.path.join(outdir, 'SupplyTechsCapitalCost.csv'), 'SupplyTechsCapitalCost')
    StorageTechsCapacityCapitalCost = _read_csv(os.path.join(outdir, 'StorageTechsCapacityCapitalCost.csv'), 'StorageTechsCapacityCapitalCost')
    StorageTechsEnergyCapitalCost = _read_csv(os.path.join(outdir, 'StorageTechsEnergyCapitalCost.csv'), 'StorageTechsEnergyCapitalCost')
    StorageTechsEnergyCapitalCost.rename(columns={'energy_unit': 'capacity_or_energy_unit'}, inplace=True)

    dfs = [SupplyTechsCapitalCost, StorageTechsCapacityCapitalCost, StorageTechsEnergyCapitalCost]
    merged = pd.concat(dfs, keys=['capacity', 'capacity_', 'energy'], names=['capacity_or_energy'])
    merged = merged.reset_index().drop('level_1', axis='columns')
    merged.replace({'capacity_':'capacity'}, inplace=True)

    column_order = [u'supply_tech', 'capacity_or_energy', u'definition', u'reference_tech', u'geography', u'currency', u'currency_year', 'cost_of_capital', u'capacity_or_energy_unit', u'time_unit', u'is_levelized', u'interpolation_method',
    u'extrapolation_method', u'extrapolation_growth', u'source', u'notes', u'geography_map_key', u'new_or_replacement', u'gau', u'demand_sector', u'resource_bin', u'vintage', u'value',
    u'sensitivity']
    merged = merged[column_order]
    merged_path = os.path.join(outdir, 'SupplyTechsCapitalCost.csv')
    merged.to_csv(merged_path, index=None)

    os.remove(os.path.join(outdir, 'StorageTechsCapacityCapitalCost.csv'))
    os.remove(os.path.join(outdir, 'StorageTechsEnergyCapitalCost.csv'))

def combine_tech_data(dbdir, tech_cost_tables):
    # Needs 2 new columns in SupplyTechsCapitalCost: new_or_replacement and capacity_or_energy
    #
    # pd.concat([new_df, replacement_df], keys=['new', 'replacement'], names=['new_or_replacement'])
    #
    # Where new_df is merging CapitalCost to CapitalCostNewData and replacement_df is merging CapitalCost
    # to CapitalCostReplacementData, both with a right join to keep only the data in the Data tables

    data_suffixes = ['NewData', 'ReplacementData']

    for table in tech_cost_tables:
        dfs = []
        for suffix in data_suffixes:
            path = os.path.join(dbdir, table + suffix + '.csv')
            dfs.append(_read_csv(path, table))

        merged = pd.concat(dfs, keys=['new', 'replacement'], names=['new_or_replacement'])
        merged = merged.reset_index().drop('level_1', axis='columns')
        merged_path = os.path.join(dbdir, table + 'Data.csv')
        merged.to_csv(merged_path, index=None)

@click.command()

@click.option('--dbdir', '-d', type=click.Path(exists=True, file_okay=False), default="database.csvdb",
              help='Path to the database directory. (Default is "database.csvdb")')

@click.option('--outdir', '-o',
              help='Where to write the merged CSV files.')

@click.option('--metadata-file', '-m', default='./_GeneratedDatabase.py',
              help='''Path to the python file to create containing metadata and database class. Default is ./_GeneratedDatabase.py. Set to "" to skip writing the file.''')

@click.option('--classname', '-c',
              help='Name of the CsvDatabase subclass to create with generated metadata. Default is basename of metadata-file.')

@click.option('--shapes/--no-shapes', default=True,
              help='Whether to copy the ShapeData directory to the merged database. (Default is --shapes)')

def main(dbdir, outdir, metadata_file, classname, shapes):
    mkdirs(outdir)

    # Before we merge, we combine the "new" + "replacement" data into a single *Data file
    # and delete the two original files.
    tech_cost_tables = ['SupplyTechsInstallationCost', 'SupplyTechsCapitalCost', 'StorageTechsCapacityCapitalCost', 'StorageTechsEnergyCapitalCost']
    combine_tech_data(dbdir, tech_cost_tables)

    csvFiles = glob(os.path.join(dbdir, '*.csv'))
    tables   = map(lambda path: os.path.basename(path)[:-4], csvFiles)
    children = filter(lambda x: x.endswith('Data'), tables)

    metadata = {}

    # exceptions to "data_table" rule
    not_data_table = ['DispatchNodeConfig']

    for tbl_name in tables:
        if not shapes and tbl_name=='Shapes':
            continue

        child_name = tbl_name + 'Data'
        if not child_name in children:
            child_name = tbl_name + 'NewData'

        if child_name in children:
            md = denormalize(dbdir, outdir, tbl_name, child_name)
            metadata[tbl_name] = md

        elif not ((tbl_name.endswith('Data') and tbl_name != 'BlendNodeInputsData') or tbl_name in Simple_mapping_tables):
            # Copy files that aren't *Data and that don't have related *Data files,
            # and aren't simple mapping tables, which aren't needed in the csvdb
            basename = tbl_name + '.csv'
            print("Copying", basename)
            src = os.path.join(dbdir, basename)
            dst = os.path.join(outdir, basename)
            shutil.copy2(src, dst)

            if tbl_name in not_data_table:
                key_col = get_key_col(dbdir, tbl_name)
                md = {'key_col' : key_col}
            else:
                md = {'data_table': True}

            metadata[tbl_name] = md

    combine_techs(outdir)

    if metadata_file:
        gen_database_file(metadata_file, metadata, classname)

    if shapes:
        src = os.path.join(dbdir,  'ShapeData')
        dst = os.path.join(outdir, 'ShapeData')
        if os.path.exists(dst):
            shutil.rmtree(dst)

        print("Copying ShapeData")
        shutil.copytree(src, dst)

if __name__ == '__main__':
    main()
